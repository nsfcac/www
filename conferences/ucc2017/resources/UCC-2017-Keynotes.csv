Keynote Number,Day,Time,Speaker,Position and Affiliation,Title and Link to Abstract,Abstract,Bio,Panels,Notes
1,Wednesday Dec. 6,8:00-9:00 am,Kate Keahey,Scientist at Argonne National Laboratory and Senior Fellow at the Computation Institute at the University of Chicago,<a href=keynotes/keynote1.php>Towards an Experimental Instrument for Computer Science Research</a>,"While the types of Computer Science experiments we can imagine are limited only by our creativity, in practice we can carry out only those that are supported by an instrument that allows us to deploy, capture, and record relevant scientific phenomena. Over the last several years Computer Science testbeds such as GENI and NSFCloud have significantly contributed towards developing experimental testbeds that make the deployment of a diversity of experiments feasible. What features are needed to transform those testbeds into scientific instruments that enable us to easily and accurately capture, and record as well as deploy? What type of support and features do we need for the support of repeatability and reproducibility? What can Computer Science testbeds do to improve experimental methodology?  In this presentation I will introduce Chameleon, an NSFCloud open, deeply reconfigurable testbed supporting Computer Science experimentation at large scale. With 15,000+ cores, 5 PB of storage, and sites connected by 100G networks – and now extended for phase 2 which will bring even more exciting capabilities to its thousands of users – Chameleon is uniquely positioned to provide tools and encourage practices leading to better Computer Science experiments. I will describe the system, discuss new hardware and features that we will introduce in the coming years, and give examples of experiments our users are running. I will then describe support how we will leverage the testbed features to provide support for experimental methodology and repeatability in the current phase of system development. 
","Kate Keahey is one of the pioneers of infrastructure cloud computing. She created the Nimbus project, recognized as the first open source Infrastructure-as-a-Service implementation, and engages in many application projects enabling the use of the cloud computing platform in science. She is currently leading the Chameleon project, a large scale, deeply reconfigurable experimental testbed for cloud computing research and a co-founder of SoftwareX, a new journal format established to publish software contributions. Kate is a Scientist at Argonne National Laboratory and a Senior Fellow at the Computation Institute at the University of Chicago. 

",Tuesday afternoon DAAC,Picture at http://www.mcs.anl.gov/~keahey/pictures/Kate-small.png
2,Wednesday Dec. 6,9:00-10:00 am,David Hancock,"Program Director for Advanced Cyberinfrastructure, Pervasive Technology Institute Research Technologies Division, Indiana University","<a href=keynotes/keynote2.php>Jetstream - Early Operations Performance, Adoption, and Impacts</a>","Jetstream1, built with OpenStack, is the first production cloud funded by the NSF for conducting general-purpose science and engineering research as well as an easy-to-use platform for education activities. Unlike many high-performance computing systems, Jetstream uses the interactive Atmosphere graphical user interface developed as part of the iPlant (now CyVerse) project and focuses on interactive use on uniprocessor or multiprocessor. This interface provides for a lower barrier of entry for use by educators, students, practicing scientists, and engineers. A key part of Jetstream’s mission is to extend the reach of the NSF’s eXtreme Digital (XD) program to a community of users who have not previously utilized NSF XD program resources, including those communities and institutions that traditionally lack significant cyberinfrastructure resources. OpenStack deployments all have the same five basic services: identity, images, block storage, networking, and compute. There are additional services offered; however, by and large, they are underutilized. The use of these services will be discussed as well as highlights from the first year of production operations, and future plans for the project.","David Hancock is the program director for advanced cyberinfrastructure within the Indiana University Pervasive Technology Institute's Research Technologies division. Hancock is responsible for directing IU's local and national high-performance computing (HPC) systems and cloud resources for research. Hancock is a senior investigator and acting primary investigator for the Jetstream project funded by the National Science Foundation (NSF). He is also responsible for directing IU system administrators who participate in the NSF XSEDE project as well as being senior personnel on a number of other completed NSF projects.
Hancock is also an active participant in a number of national and international HPC organizations and is currently serving as the president of the Cray User Group and a representative in the XSEDE Service Provider (SP) Forum. Previously he served as the vice president for the IBM HPC User Group, vice president and program chair for the Cray User Group, and vice chair of the XSEDE SP Forum.
",,
3,Wednesday Dec. 6,1:00-2:00 pm,Richard Sinnott,"Director of eResearch and Professor of Applied Computing Systems, University of Melbourne",<a href=keynotes/keynote3.php>Cloud Trek - The Next Generation</a>,"Educating the next generation of software engineers is essential with the increased move to an Internet-based society. The need to support big data and data analytics are challenging many of the typical scenarios and paradigms associated with software engineering. In the digital age, data is often messy, distributed and growing exponentially. In this context there are swathes of technologies that are shaping the landscape for dealing with these phenomenon. Cluster and high performance computing has been a core approach for processing larger scale data sets, but Cloud computing has now gained increasing prominence and acceptance. In this context, training and educating the next generation of software engineers to be savvy Cloud application developers is essential. Prof Sinnott has taught Cluster (HPC) and Cloud Computing at the University of Melbourne for 5 years and exposed students to the latest technologies for big data analytics. Many of these efforts are shaped by the portfolio of major projects utilising numerous big data technologies within the Melbourne eResearch Group (www.eresearch.unimelb.edu.au). This presentation covers the pedagogy of the course and describes the way in which it utilizes national cloud and storage resources made available across Australia. Examples of the shaping eResearch projects and the solutions developed by the students are illustrated to demonstrate the practical experiences in developing Cloud-based solutions that focus especially on ‘big data’ challenges.  
","Professor Richard O. Sinnott is the Director of eResearch at the University of Melbourne and Professor of Applied Computing Systems. In these roles he is responsible for all aspects of eResearch (research-oriented IT development) at the University. He has been lead software engineer/architect on an extensive portfolio of national and international projects, with specific focus on those research domains requiring finer-grained access control (security). Prior to coming to Melbourne, Richard was the Technical Director of the UK National e-Science Centre; Director of e-Science at the University of Glasgow; Deputy Director (Technical) for the Bioinformatics Research Centre also at the University of Glasgow, and for a while the Technical Director of the National Centre for e-Social Science. He has a PhD in Computing Science, an MSc in Software Engineering and a BSc in Theoretical Physics (Hons). He has over 300 peer-reviewed publications across a range of computing and application-specific domains. He teaches High Performance Computing and Cloud Computing at the University of Melbourne.",,
4,"Thursday, Dec. 7",8:00-9:00 am,Geoffrey Fox,"Distinguished Professor of Engineering, Computing, and Physics at Indiana University","<a href=keynotes/keynote4.php>Components and Rationale of a Big Data Toolkit Spanning HPC, Grid, Edge and Cloud Computing</a>","We look again at Big Data Programming environments such as Hadoop, Spark, Flink, Heron, Pregel; HPC concepts such as MPI and Asynchronous Many-Task runtimes and Cloud/Grid/Edge ideas such as event-driven computing, serverless computing, workflow, and Services. These cross many research communities including distributed systems, databases, cyberphysical systems and parallel computing which sometimes have inconsistent worldviews. There are many common capabilities across these systems which are often implemented differently in each packaged environment. For example, communication can be bulk synchronous processing or data flow; scheduling can be dynamic or static; state and fault-tolerance can have different models; execution and data can be streaming or batch, distributed or local. We suggest that one can usefully build a toolkit (called Twister2 by us) that supports these different choices and allows fruitful customization for each application area. We illustrate the design of Twister2 by several point studies. We stress the many open questions in very traditional areas including scheduling, messaging and checkpointing.","Fox received a Ph.D. in Theoretical Physics from Cambridge University where he was Senior Wrangler. He is now a distinguished professor of Engineering, Computing, and Physics at Indiana University where he is director of the Digital Science Center, and both Department Chair and Interim Associate Dean for Intelligent Systems Engineering at the School of Informatics, Computing, and Engineering.  He previously held positions at Caltech, Syracuse University, and Florida State University after being a postdoc at the Institute for Advanced Study at Princeton, Lawrence Berkeley Laboratory, and Peterhouse College Cambridge. He has supervised the Ph.D. of 70 students and published around 1300 papers (over 450 with at least 10 citations) in physics and computing with an hindex of 76 and over 31000 citations. He is a Fellow of APS (Physics) and ACM (Computing) and works on the interdisciplinary interface between computing and applications.
",,
5,"Thursday, Dec. 7",1:00-2:00 pm,Dan Stanzione,"Executive Director, Texas Advanced Computing Center, University of Texas at Austin","<a href=keynotes/keynote5.php>HPC, Cloud Services and the Evolution of Scientific Computing</a>","High Performance Computing has been the backbone of scientific computing for decades.  Driven primarily by simulation, larger and larger computing systems have been built to solve problems of incredible scale and complexity.  The rise of the web and cloud computing has not replaced these large scale engines, but rather has augmented them, allowing increasingly complex scientific workflows using HPC in conjunction with cloud technologies -- containers, Rest APIs, manages services, etc.   In this talk, I will discuss how the Texas Advanced Computing Center (TACC) at UT Austin is responding to these trends; both in the way we design, deploy and support our large scale HPC systems, and how the science we support uses an ever increasing array of cyberinfrastructure services. ","Dr. Stanzione is the Executive Director of the Texas Advanced Computing Center (TACC) at The University of Texas at Austin since July 2014, previously serving as Deputy Director. He is the principal investigator (PI) for a National Science Foundation (NSF) grant to deploy and support Stampede2, a large scale supercomputer, which will have over twice the system performance of TACC’s original Stampede system. Stanzione is also the PI of TACC's Wrangler system, a supercomputer for data-focused applications. For six years he was co-director of CyVerse, a large-scale NSF life sciences cyberinfrastructure. Stanzione was also a co-principal investigator for TACC's Ranger and Lonestar supercomputers, large-scale NSF systems previously deployed at UT Austin. Stanzione received his bachelor's degree in electrical engineering and his master's degree and doctorate in computer engineering from Clemson University.
",,
6,"Friday, Dec. 8",8:00-9:00 am,Wo Chang,"Digital Data Advisor, Information Technology Laboratory, National Institute of Standards and Technology",<a href=keynotes/keynote6.php>NIST Big Data Reference Architecture for Analytics and Beyond</a>,"Big Data is the term used to describe the deluge of data in our networked, digitized, sensor-laden, information driven world.  There is a broad agreement among commercial, academic, and government leaders about the remarkable potential of “Big Data” to spark innovation, fuel commerce, and drive progress. The availability of vast data resources carries the potential to answer questions previously out of reach. However, there is also broad agreement on the ability of Big Data to overwhelm traditional approaches. 

Big Data architectures come in many shapes and forms ranging from academic research settings to product-oriented workflows.  With massive-scale dynamic data being generate from social media, Internet of Things, Smart Cities, and others, it is critical to analyze these data in real-time and provide proactive decision.  With the advancement of computer architecture in multi-cores and GPUs, and fast communication between CPUs and GPUs, parallel processing utilizes these platforms could optimize resources at a reduced time. 

This presentation will provide the past, current, and future activities of the NIST Big Data Public Working Group (NBD-PWG) and how the NIST Reference Architecture may address the rate at which data volumes, speeds, and complexity are growing requires new forms of computing infrastructure to enable Big Data analytics interoperability such that analytics tools can be re-usable, deployable, and operational.  

The focus of NBD-PWG is to form a community of interest from industry, academia, and government, with the goal of developing consensus definitions, taxonomies, secure reference architectures, and standards roadmap which would create vendor-neutral, technology and infrastructure agnostic framework.  The am is to enable Big Data stakeholders to pick-and-choose best analytics tools for their processing under the most suitable computing platforms and clusters while allowing value-additions from Big Data service providers and flow of data between the stakeholders in a cohesive and secure manner.","Mr. Wo Chang is Digital Data Advisor for the NIST Information Technology Laboratory (ITL). His responsibilities include working with data interoperability; promoting a vital and growing Big Data community at NIST and with external stakeholders in the commercial, academic, and government sectors. Mr. Chang currently the Convener of the ISO/IEC JTC 1/WG9 Working Group on Big Data, co-chairs the NIST Big Data Public Working Group, and chairs the IEEE Big Data Governance and Metadata Management. 

In the past, Mr. Chang was manager of the Digital Media Group in ITL and his duties included oversees several key projects including digital data archival and preservation, management of electronic health records, motion image quality, and multimedia standards.  Chang was the Deputy Chair for the US INCITS L3.1, chaired other key projects for MPEG, participated with HL7 and ISO/IEC TC215 for health informatics, IETF for the protocols development, and was one of the original members of the W3C's SMIL and developed one of the SMIL reference software.  Mr. Chang’s research interests include many CPUs/Cores/GPUs high performance analytics and computing, scalable real-time graph mining algorithms and visual analytics for massive audiovisual content, digital data mashup, cloud computing, content metadata description, multimedia synchronization, and Internet protocols.
",,